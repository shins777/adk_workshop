# Copyright 2025 Forusone(forusone777@gmail.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from google.genai import types 
from typing import Optional
from google.adk.agents.callback_context import CallbackContext
from google.adk.models import LlmResponse, LlmRequest

#--------------------------------[callback_before_model]----------------------------------

def callback_before_model(callback_context: CallbackContext, 
                         llm_request: LlmRequest
                         ) -> Optional[LlmResponse]:
    """
    Pre-processing callback executed before the LLM model is called.

    This function checks the user's last message for the presence of a specific keyword defined in the agent's state.
    If the keyword is detected in the user's query, it blocks the LLM call and returns a custom response message.
    Otherwise, it allows the LLM call to proceed by returning None.

    Args:
        callback_context (CallbackContext): The context containing agent information and state.
        llm_request (LlmRequest): The request containing user input and message contents.

    Returns:
        Optional[LlmResponse]: A custom LlmResponse if the keyword is detected, or None to proceed with the LLM call.
    """
    
    # Get the contextual information from CallbackContext
    agent_name = callback_context.agent_name
    current_state = callback_context.state.to_dict()
    keyword = current_state.get("keyword").lower()

    # Inspect the last user message in the request contents
    last_user_message = ""
    if llm_request.contents and llm_request.contents[-1].role == 'user':
         if llm_request.contents[-1].parts:
            last_user_message = llm_request.contents[-1].parts[0].text.lower()

    print(f"[Before Model] Agent Name : {agent_name} Keyworkd : {keyword} - Last user message: '{last_user_message}'")

    # Check the keyword in the user's message that could be undesirable keyword.
    if keyword in last_user_message:        
        print(f"[Before Model] Asking with {keyword} keyword was detected in user's query. Skipping LLM call.")
        return LlmResponse(
            content=types.Content(
                role="model",
                parts=[types.Part(text="LLM call was blocked by callback function since keyword was not allowed to use.")],
            )
        )
    else:
        print(f"[Before Model] No specific {keyword} keyword was detected to be violated in user's query, proceeding with LLM call.")
        return None

#--------------------------------[callback_after_model]----------------------------------
    
def callback_after_model(callback_context: CallbackContext, 
                        llm_response: LlmResponse
                        ) -> Optional[LlmResponse]:
    """
    Post-processing callback executed after the LLM model generates a response.

    This function inspects the LLM response for the presence of a specific keyword defined in the agent's state.
    If the keyword is found in the response, it blocks the original LLM response and returns a custom message instead. Otherwise, it allows the response to proceed as normal.

    Args:
        callback_context (CallbackContext): The context containing agent information and state.
        llm_response (LlmResponse): The response generated by the LLM model.

    Returns:
        Optional[LlmResponse]: A custom LlmResponse if the keyword is detected, or None to proceed with the original response.
    """

    # Get the contextual information from CallbackContext
    agent_name = callback_context.agent_name
    current_state = callback_context.state.to_dict()
    keyword = current_state.get("keyword").lower()

    llm_response_message = ""
    if llm_response.content and llm_response.content.parts:
        llm_response_message = llm_response.content.parts[0].text.lower()

    print(f"[After Model] Agent Name : {agent_name} Keyworkd : {keyword} - Inspecting last model message: '{llm_response_message}'")

    if keyword in llm_response_message:

        print(f"[After Model] {keyword} keyword found in AI response. don't reply the received model response to user.")
        return LlmResponse(
            content=types.Content(
                role="model",
                parts=[types.Part(text=f"Response of model call was blocked by callback_after_model due to {keyword} found in model response.")],
            )
        )
    else:
        print(f"[After Model] No specific {keyword} found in model response. Proceeding with response of model call.")
        return None
